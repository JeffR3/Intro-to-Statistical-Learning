{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Learning\n",
    "\n",
    "## What is Statistical Learning\n",
    "- Imagine you're hired to provide advice on how to improve sales of a product\n",
    "- The **Advertising** data set contains sales of that product in different markets along with advertising budget of the product in 3 different media\n",
    "    - TV\n",
    "    - Radio\n",
    "    - Newspaper\n",
    "- It is impossible to directly increase sales but its possible indirectly\n",
    "- In this setting sales is considered the _output variable_ and advertising budgets are _input variables_\n",
    "- Input variables can also be named\n",
    "    - Independent variables\n",
    "    - Features\n",
    "    - Variables\n",
    "- Output variable can also be named\n",
    "    - Response\n",
    "    - Dependent Variable\n",
    "<img src=\"./Figures/Chapter2/2.1.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- We can assume there is a relationship between output and input\n",
    "- This can be written in the general form of:<br>    \n",
    "$$Y = f(X) +\\epsilon $$\n",
    "\n",
    "- $f(X)$ is a fixed but unknown function of $X$\n",
    "- $\\epsilon$ is a random _error term_, independent of X with a mean of 0\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.2.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- The plot suggest predicting income based on **years of education**\n",
    "- The function $f(X)$ may involve more than 1 feature\n",
    "<img src=\"./Figures/Chapter2/2.3.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- Statistical learning refers to set of approaches for estimating $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Estimate $f$ ?\n",
    "- There are 2 main reasons to estimate $f$\n",
    "    - Prediction\n",
    "    - Inference\n",
    "\n",
    "#### Prediction\n",
    "- A set of inputs are readily available but output cannot be obtained\n",
    "- We can predict $Y$ as:\n",
    "$$\\hat Y = \\hat f(X)$$\n",
    "\n",
    "- $\\hat f$ represents our estimate for $f$ \n",
    "- $\\hat Y$ represents our prediction for $Y$\n",
    "- $\\hat f$ is considered a black box, in the sense that one is not concerned with the form of $\\hat f$ as long as it got accurate prediction for $Y$\n",
    "- Imagine $X$ are characteristics of a patients blood\n",
    "- $Y$ in this case, is endocding of the patient's risk for a severe adverse reaction to a particular drug\n",
    "- Predict $Y$ using $X$ to avoid giving the drug to high estimate of $Y$\n",
    "- Accuracy of $\\hat Y$ depends on 2 quantities:\n",
    "    - Reducible error\n",
    "    - Irreducible error\n",
    "- $\\hat f$ will never be perfect estimate of $f$ producing error\n",
    "- This is a _reducible error_ because accuracy is improved by statistical learning technique\n",
    "- If it became the perfect estimate of $$\\hat Y=f(X)$$ our prediction would still have an error\n",
    "- $Y$ is also a function of $\\epsilon$\n",
    "- We cannot reduce the error introduced by $\\epsilon$ making it _irreducible_\n",
    "- $\\epsilon$ may contain unmeasured variables that are usful for predicting $Y$ but since it's not measured $f$ cannot use them\n",
    "- In the scenario above, the risk of adverse reaction may vary for a given patient on a given\n",
    "- Consider $\\hat Y = \\hat f(X)$ with $\\hat f$ and $X$ are fixed\n",
    "$$E(Y-\\hat Y)^2 = E[ f(X)+ \\epsilon -\\hat f(X) ]^2$$\n",
    "$$= [f(X) - \\hat f(X)]^2+Var( \\epsilon )$$\n",
    "- The $E(Y-\\hat Y)^2$ represnts the average or _expected value_, of the squared difference between predicted and actual value of $Y$\n",
    "- The $Var(\\epsilon)$ represents the variance associated with the error term $\\epsilon$\n",
    "- The goal is estimating $f$ with the aim of lowering reducible error\n",
    "- The irreducible error will always be upper bound and unknown in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inference\n",
    "\n",
    "- We are often interested how $Y$ is affected as $X_p$ change\n",
    "- We want to understand the relationship between $X$ and $Y$\n",
    "- $\\hat f$ cannot be treated a black box since we need exact form\n",
    "- Usually interested in answering the questions:\n",
    "    - Which predictors are associated with the response ?\n",
    "    - What is the relationship between the response and each predictor ?\n",
    "    - Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation or is it more complicated?\n",
    "- The **Advertising** data set might be interested in answering\n",
    " questions such as :\n",
    "    - Which media contribute to sales?\n",
    "    - Which media generate the biggest boost in sales?\n",
    "    - How much increase in sales is associated with a given increase in a certain media?\n",
    "- This is an example of the inference paradigm\n",
    "- Some modeling could be conducted for prediction and inference\n",
    "- Different methods for estimating $f$ depend on goal\n",
    "- _Linear models_ allow simple and interpretable inference but not accurate predictions\n",
    "- The reverse is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How Do We Estimate $f$ ?\n",
    "\n",
    "- Explore linear and non-linear approaches for estimating $f$\n",
    "- These methods generally share certain characteristics\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.2.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "- These observation are called the _training data_\n",
    "- The goal is to apply a statistical learning method to the training data to estimate $f$\n",
    "- Find function $\\hat f$ that $Y \\approx \\hat f(X)$ for any observation $(X,Y)$\n",
    "- Most statistical learning methods can be characterized as:\n",
    "    - Parametric\n",
    "    - Non-parametric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parametric Methods\n",
    "\n",
    "- Involves 2-step model-based approach\n",
    "\n",
    "- Assume the functional form of $f$\n",
    "    - E.g. $f$ is linear in $X$:\n",
    "$$f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .. + \\beta_p X_p$$\n",
    "\n",
    "- Once the assumption is made, estimating $f$ is simplified\n",
    "- Estimate _p_-dimensional function $f(X)$ became estimate _p_+1 coeffcients\n",
    "\n",
    "- After the model has been selected, We then _fit_ or _train_ the model\n",
    "- In the case of the linear model, we estimate the parameters $\\beta_0, \\beta_1, \\beta_2,...., \\beta_p$\n",
    "- We want values for the parameters such that:\n",
    "$$Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .. + \\beta_p X_p$$\n",
    "\n",
    "- The most common approach to fitting the model is _(Ordinary) least squares_\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.4.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- This approach is referred to as parametric: Reducing the problem of estimating $f$ down to estimating a set of parameters\n",
    "- The disadvantage of parametric approach is that the model will usually not match the true $f$\n",
    "- If the model is far from $f$, our estimate will be poor\n",
    "- We can fit a flexible model but the flexible model requires estimating a greater number of parameters\n",
    "- These complex models can lead to _overfitting_ the data, which follow the errors or _noise_ too closely\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.3.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- The parametric approach of this is \n",
    "$$Income \\approx \\beta_0 + \\beta_1 * education + \\beta_2 * seniority $$\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.4.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- We assume a linear relationship between response and 2 predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Non-parametric Methods\n",
    "\n",
    "- Unlike parametric methods, Non-parametrics methods do not make explicit assumptions about $f$\n",
    "- Seek an estimate of $f$ that gets close to data point as possible\n",
    "- Accurately fit a wider range of posssible shapes for $f$\n",
    "- The disadvantage is that it requires a large number of observations is required to obtain accurate estimate for $f$\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.3.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.5.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- A _thin-plate spline_ is used to estimate $f$ as close as possible to observed data being _smooth_\n",
    "- To fit a thing-plate spline, the data analyst must select a level of smoothness\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.5.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.6.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- The rough thin-plate spline is more variable then $f$, showing overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trade-Off between Prediction Accuracy and Model Interpretability\n",
    "\n",
    "- Different models have different flexibility\n",
    "\n",
    "- Why would we choose to use a more restrictive method instead of a very flexible approach?\n",
    "- If mainly interested in inference, restrictive models are more interpretable\n",
    "- Flexible models can lead to complicated estimates of $f$\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.7.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- When Inference is the goal, simple and inflexible methods is best\n",
    "- Otherwise if interested in prediction, the interpretability of the predictive model is not of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised V. Unsupervised Learning\n",
    "\n",
    "- Statistical learning problems fall into 2 categories\n",
    "    - Supervised\n",
    "    - Unsupervised\n",
    "- Previous examples are in the supervised domain\n",
    "- For each $x_i$ there is a response $y_i$\n",
    "- Majority of problems will be supervised\n",
    "\n",
    "- Unsupervised learning is more challenging\n",
    "- For each $x_i$, this is no $y_i$\n",
    "- Used to understand the relationships between the variables \n",
    "- One method is _cluster analysis_, with the goal whether the observations fall into distinct groups\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.8.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- Clustering methods cannot be expected to assign to correct group\n",
    "- We cannot easily plot the observation if more than 2 variables\n",
    "\n",
    "- Analysis considered supervised or unsupervised is less clear-cut\n",
    "- If you have $n$ observation and $m$ out of the the $n$ observation while $m < n$ have response measurement, and the remaing $n - m$ have no response, this is considered _semi-supervised learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression V. Classification Problems\n",
    "\n",
    "- Variables can be categorized as either quantitative or qualitative\n",
    "- Quantitative problems are considered regression problems while qualitive response are referred as classification problems\n",
    "- The distinction is not always clear in the case of using logistic regression used with qualitative response\n",
    "- Whether the features are qualitative or quantitative are considered less important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing Model Accuracy\n",
    "\n",
    "- It is necessary to introduce different statistical learning methods since there is no best method\n",
    "- There will be important concepts that arise in selecting a statistical learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measuring the Quality of Fit\n",
    "\n",
    "- Evaluate the performance of statistical learning method on a given data set\n",
    "- The most commonly-used measure is the _mean squared error(MSE)_\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat f(x_i))^2 $$\n",
    "\n",
    "- MSE will be small if predicted close to true responses,otherwise it will be large\n",
    "- MSE uses the training data to was used to fit the model making it _training MSE_\n",
    "- We are more interested in the accuracy of the predictions on unseen data\n",
    "- Because of this we want the lowest _test MSE_ therefore\n",
    "\n",
    "$$Ave(y_0 - \\hat f(x_0))^2 $$\n",
    "\n",
    "- The average squared prediction error for the test observation\n",
    "- Evaluate on the test observation \n",
    "- If no test observation are available, then use the train MSE since it is closely related to test MSE\n",
    "- The problem is that lowest train MSE does not guarantee lowest test MSE\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.9.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "- The _degrees of freedom_ represents the number of smoothing splines\n",
    "- A fundamental property of statistical learning is that a train MSE will have a monotone decrease while test MSE will have a U-shape\n",
    "- When a model has small train MSE but large test MSE then we are _overfitting_\n",
    "- The train MSE will always be lower than test MSE since statistical learning methods either directly or indirectly seek to minimize the training MSE\n",
    "\n",
    "<img src=\"./Figures/Chapter2/2.10.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "- Test MSE is more difficult because no test data is available\n",
    "- A variety of approaches to estimate minimum point\n",
    "- One method is _cross-validation_, a method for estimating test MSE using traing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias-Variance Trade-Off\n",
    "\n",
    "- The U-shape observed in the test MSE curves turns to be the result of 2 competing properties of statistical learning methods\n",
    "- The expected test MSE for a given value $x_0$ can be decomposed into the sum of 3 fundamental quantites\n",
    "    - Variance of $\\hat f(x_0)$\n",
    "    - Squared Bias of $\\hat f(x_0)$\n",
    "    - Variance of the error terms $\\epsilon$\n",
    "$$ E(y_0 - \\hat f(x_0))^2 = Var( \\hat f(x_0)) + [Bias(\\hat f(x_0))]^2 + Var(\\epsilon)$$\n",
    "\n",
    "- The notation $E(y_0 - \\hat y (x_0))^2$ defines the _expected test MSE_ and refers to average test MSE if we estimated $f$ using large number of training sets and tested at $x_0$\n",
    "- To minimize the expected test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Classification Setting\n",
    "\n",
    "#### Bayes Classifer\n",
    "#### K-Nearest Neighbors\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
